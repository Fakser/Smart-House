# Lexer

> Auto-generated documentation for [env.Lib.site-packages.jinja2.lexer](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py) module.

Implements a Jinja / Python combination lexer. The ``Lexer`` class
is used to do some preprocessing. It filters out invalid operators like
the bitshift operators we don't allow in templates. It separates
template code and python code in expressions.

- [Smart-house-rest-api](..\..\..\..\README.md#description) / [Modules](..\..\..\..\MODULES.md#smart-house-rest-api-modules) / `Env` / `Lib` / `Site-packages` / [Jinja2](index.md#jinja2) / Lexer
    - [Failure](#failure)
    - [Lexer](#lexer)
        - [Lexer().tokeniter](#lexertokeniter)
        - [Lexer().tokenize](#lexertokenize)
        - [Lexer().wrap](#lexerwrap)
    - [OptionalLStrip](#optionallstrip)
    - [Token](#token)
        - [Token().test](#tokentest)
        - [Token().test_any](#tokentest_any)
    - [TokenStream](#tokenstream)
        - [TokenStream().\_\_next\_\_](#tokenstream__next__)
        - [TokenStream().close](#tokenstreamclose)
        - [TokenStream().eos](#tokenstreameos)
        - [TokenStream().expect](#tokenstreamexpect)
        - [TokenStream().look](#tokenstreamlook)
        - [TokenStream().next_if](#tokenstreamnext_if)
        - [TokenStream().push](#tokenstreampush)
        - [TokenStream().skip](#tokenstreamskip)
        - [TokenStream().skip_if](#tokenstreamskip_if)
    - [TokenStreamIterator](#tokenstreamiterator)
    - [compile_rules](#compile_rules)
    - [count_newlines](#count_newlines)
    - [describe_token](#describe_token)
    - [describe_token_expr](#describe_token_expr)
    - [get_lexer](#get_lexer)

#### Attributes

- `whitespace_re` - static regular expressions: `re.compile('\\s+', re.U)`
- `name_re` - Python 2, no Unicode support, use ASCII identifiers: `re.compile('[a-zA-Z_][a-zA-Z0-9_]*')`
- `TOKEN_ADD` - internal the tokens and keep references to them: `intern('add')`
- `operators` - bind operators to token types: `{'+': TOKEN_ADD, '-': TOKEN_SUB, '/': TOKEN_DIV...`

## Failure

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L246)

```python
class Failure(object):
    def __init__(message, cls=TemplateSyntaxError):
```

Class that raises a `TemplateSyntaxError` if called.
Used by the [Lexer](#lexer) to specify known errors.

## Lexer

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L459)

```python
class Lexer(object):
    def __init__(environment):
```

Class that implements a lexer for a given environment. Automatically
created by the environment class, usually you don't have to do that.

Note that the lexer is not automatically bound to an environment.
Multiple environments can share the same lexer.

### Lexer().tokeniter

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L662)

```python
def tokeniter(source, name, filename=None, state=None):
```

This method tokenizes the text and returns the tokens in a
generator.  Use this method if you just want to tokenize a template.

### Lexer().tokenize

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L613)

```python
def tokenize(source, name=None, filename=None, state=None):
```

Calls tokeniter + tokenize and wraps it in a token stream.

### Lexer().wrap

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L618)

```python
def wrap(stream, name=None, filename=None):
```

This is called with the stream as returned by [Lexer().tokenize](#lexertokenize) and wraps
every token in a :class:[Token](#token) and converts the value.

## OptionalLStrip

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L446)

```python
class OptionalLStrip(tuple):
```

A special tuple for marking a point in the state that can have
lstrip applied.

## Token

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L259)

```python
class Token(tuple):
```

Token class.

### Token().test

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L275)

```python
def test(expr):
```

Test a token against a token expression.  This can either be a
token type or ``'token_type:token_value'``.  This can only test
against string values and types.

### Token().test_any

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L288)

```python
def test_any(*iterable):
```

Test against multiple token expressions.

## TokenStream

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L320)

```python
implements_iterator
class TokenStream(object):
    def __init__(generator, name, filename):
```

A token stream is an iterable that yields :class:[Token](#token)\s.  The
parser however does not iterate over it but calls :meth:`next` to go
one token ahead.  The current active token is stored as attribute `current`.

### TokenStream().\_\_next\_\_

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L377)

```python
def __next__():
```

Go one token ahead and return the old one.

Use the built-in :func:`next` instead of calling this directly.

### TokenStream().close

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L392)

```python
def close():
```

Close the stream.

### TokenStream().eos

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L344)

```python
@property
def eos():
```

Are we at the end of the stream?

### TokenStream().expect

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L398)

```python
def expect(expr):
```

Expect a given token type and return it.  This accepts the same
argument as :meth:`jinja2.lexer.Token.test`.

### TokenStream().look

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L353)

```python
def look():
```

Look at the next token.

### TokenStream().next_if

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L366)

```python
def next_if(expr):
```

Perform the token test and return the token if it matched.
Otherwise the return value is `None`.

### TokenStream().push

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L349)

```python
def push(token):
```

Push a token back to the stream.

### TokenStream().skip

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L361)

```python
def skip(n=1):
```

Got n tokens ahead.

### TokenStream().skip_if

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L373)

```python
def skip_if(expr):
```

Like :meth:[TokenStream().next_if](#tokenstreamnext_if) but only returns `True` or `False`.

## TokenStreamIterator

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L299)

```python
implements_iterator
class TokenStreamIterator(object):
    def __init__(stream):
```

The iterator for tokenstreams.  Iterate over the stream
until the eof token is reached.

## compile_rules

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L205)

```python
def compile_rules(environment):
```

Compiles all the rules from the environment into a list of rules.

## count_newlines

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L198)

```python
def count_newlines(value):
```

Count the number of newline characters in the string.  This is
useful for extensions that filter a stream.

## describe_token

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L180)

```python
def describe_token(token):
```

Returns a description of the token.

## describe_token_expr

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L187)

```python
def describe_token_expr(expr):
```

Like [describe_token](#describe_token) but for token expressions.

## get_lexer

[[find in source code]](..\..\..\..\..\env\Lib\site-packages\jinja2\lexer.py#L423)

```python
def get_lexer(environment):
```

Return a lexer which is probably cached.
